METHODS

Modeling algorithms
We define the term ‘modeling algorithm’ as the collection of steps taken to develop a prediction model. We compared modeling algorithms based on the following characteristics: (1) number of predictors used in the model, (2) method to select the specified number of predictors, (3) method to develop a prediction model using the selected predictors. We considered seven options for the number of predictors, three options for variable selection, and four options for model development (Figure 1). All possible combinations of these options were assessed, leading to the evaluation of 84 unique modeling algorithms.
Methods to select predictors:
We assess contribution importance, permutation importance, and stepwise importance. Contribution importance represents fractional contribution of each predictor to the model based on the total gain of this feature’s splits (1). Higher percentage means a more important predictive feature. Permutation importance measures the importance of each predictor by measuring how much the model’s prediction error increases when the values of the predictor are randomly permuted (2). Stepwise importance follows the traditional stepwise technique that includes or excludes variables one at a time until a stopping criteria is met (3). Though it is not a valid procedure for statistical inference, stepwise modeling algorithms can be useful for prediction (4). There is no numeric importance value for stepwise importance but a subset of the k most important variables is easily obtained by using a pre-specified number of steps.
Methods to develop a risk prediction model:
Proportional hazards regression is a semi-parametric modeling technique that estimates a baseline hazard function and assumes predictors have a multiplicative effect on the hazard for an event of interest. Gradient boosting (hereafter referred to as boosting) develops an ensemble of weak prediction models (1). Each learner in the ensemble attempts to correct errors from the previous learners, and the ensemble’s prediction is the aggregate of its individual learners’ predictions. We developed boosting models using decision trees as learners (5), a technique that has been recognized in numerous settings to be state of the art for statistical prediction (6). Random forests are also ensembles of decision trees, but in contrast to boosting, the random forest comprises de-correlated decision trees that are grown using bootstrapped replicates of the original data (2, 7). We applied the standard random forest algorithm for risk prediction, which grows decision tree nodes by splitting the data based on an individual variable (8). We also applied an extension of the random forest that splits data using linear combinations of variables (9). Random forests grown using linear combinations of input variables are referred to as ‘oblique’ random forests.

Internal validation
We applied Monte-Carlo cross-validation to identify the most effective modeling algorithm for developing a final prediction model and to estimate the performance of modeling algorithms when they are applied to new data (10). Monte-Carlo cross-validation is an extension of split-sample testing, which splits available data into training and testing sets, then develops a model using the training set and validates the model in the testing set. Monte-Carlo cross-validation replicates this procedure, using a different split of the available data for each replicate, leading to a reduction in the variance of performance estimates for the modeling algorithms assessed (11). In the current analysis, we completed a total of 500 replications of split-sample testing.

Measures of model performance
Model performance was evaluated based on discrimination and calibration, as recommended by published guidelines (12, 13). Discrimination and calibration were measured using a time-dependent concordance (C-) statistic and modified D’Agostino-Nam test, respectively (14, 15). Point estimates and 95% confidence intervals (CIs) for these metrics were computed empirically using the distribution of results from Monte-Carlo cross-validation. Point estimates were the median value of a performance metric, while the 2.5th and 97.5th percentiles defined lower and upper bounds for 95% CIs, respectively. We did not find previously published prediction models for graft loss among pediatric patients, so we did not analyze net reclassification improvement as it requires a baseline prediction model (16).

Missing data
Missing values were imputed after data were split into training and testing sets during each replicate of Monte-Carlo cross-validation. All information from the testing data was withheld during this process (17). Specifically, missing values were imputed in both the training and testing data using the mean and mode of continuous and categorical variables, respectively, computed using the training data. While imputation to the mean is not appropriate for statistical inference, this technique has been shown to produce prediction models with Bayes consistency when missing values are non-informative (18).

Bayesian analysis of model performance
We applied Bayesian hierarchical models to draw inferences regarding the expected performance of modeling algorithms, accounting for correlated performance within each replicate of Monte-Carlo cross-validation (19). Specifically, we estimated the posterior probability that a given technique to fit a risk prediction model had superior discrimination or calibration compared to other techniques. All comparisons were made holding the number of predictor variables and the method to select predictor variables fixed; i.e., both models used the same predictor variables.

Statistical analysis
We conducted the current analysis following previously published guidelines on multivariable prediction models for individual prognosis and diagnosis (20). The 1 year incidence of graft loss or morality was computed overall and by transplant year. Characteristics of patients were calculated as mean with standard deviation or percent in the overall population and stratified by transplant year. Using the results from internal validation, we visually assessed the discrimination and calibration of candidate modeling algorithms to develop a final prediction model. We used posterior predicted probability to perform inference on pairwise comparisons between candidate modeling algorithms. To make results relevant for clinical settings, we restricted these pairwise comparisons to models that used 20 predictor variables. Based on the results, we selected a final prediction modeling algorithm and applied it to all the available data. We tabulated a summary of the variables included into the final prediction model - including the numeric importance value and rank along with the count and percent of how many missing values were present. We used partial dependence to estimate multivariable adjusted predicted risk as a function of each variable included in the final prediction model, separately (21).

Analyses were conducted using SAS version 9.4, R version 4.0.4, and a number of open-source R packages (22–26). All R code for the current analysis is publicly available at https://github.com/bcjaeger/graft-loss. Data for the current analysis are available by request from FILL IN.

# Integrate methodology from instructions.txt into your_project.R

# Data preparation
your_data <- clean_your_data(
  min_year = your_specific_min_year,
  predict_horizon = 360,  # Assuming 360 days prediction horizon
  time = your_time_variable,
  status = your_death_variable
)

# Select all 360 variables
all_features <- select_all_features(your_data)

# Create labels
labels <- make_labels(colname_variable = 'variable', 
                      colname_label = 'label')

# Monte Carlo cross-validation
resamples <- mc_cv_light(your_data, ntimes = 500)  # 500 replications as per methodology
mc_cv <- load_mc_cv()

# Visualize cross-validation results
fig_mc_cv_vals <- visualize_mc_cv(mc_cv)
fig_mc_cv_inf <- visualize_mc_cv_inference(mc_cv, 
                                           ftr_method = c('contribution', 'permutation', 'stepwise'),
                                           n_pred = c(5, 10, 15, 20, 25, 30, 35))  # As per 7 options for predictor numbers

# Select final features based on importance
final_features <- make_final_features(your_data, n_features = 20)  # Using 20 predictors as mentioned

# Create final model
final_recipe <- prep(make_recipe(your_data, dummy_code = FALSE))
final_data <- juice(final_recipe)

# Fit models using different methods
final_model_cph <- fit_cph(trn = final_data, vars = final_features$variables)
final_model_xgb <- fit_xgb(trn = final_data, vars = final_features$variables)
final_model_rsf <- fit_rsf(trn = final_data, vars = final_features$variables)
final_model_orsf <- fit_orsf(trn = final_data, vars = final_features$variables)

# Calculate partial dependencies
final_partial <- make_final_partial(final_model_orsf, final_data, final_features)
partial_table_data <- make_partial_table_data(final_partial, labels)

# Create tables
tbl_one <- tabulate_characteristics(your_data, labels, final_features$variables)
tbl_predictor_smry <- tabulate_predictor_smry(your_data, labels)
tbl_variables <- tabulate_missingness(final_recipe, your_data, final_features, labels)
tbl_partial_main <- tabulate_partial_table_data(partial_table_data, final_features$variables)

# Visualize feature importance
plot_feature_importance <- function(model, data) {
  # Code to visualize variable importance
}

feature_importance_plot <- plot_feature_importance(final_model_orsf, final_data)

# Summary table of most important variables
top_variables_table <- create_top_variables_table(final_model_orsf, labels)

# Evaluate model performance
performance_metrics <- evaluate_model_performance(final_model_orsf, test_data, time_dependent = TRUE)

# Bayesian analysis of model performance
bayesian_performance <- bayesian_model_comparison(list(cph = final_model_cph, 
                                                       xgb = final_model_xgb, 
                                                       rsf = final_model_rsf, 
                                                       orsf = final_model_orsf))

# Handle missing data
impute_missing_data <- function(train_data, test_data) {
  # Impute missing values using mean/mode from training data
}

# Note: Ensure to apply imputation within each fold of cross-validation

