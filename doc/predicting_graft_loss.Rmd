---
title: "Prediction of Mortality and Allograft Loss in Pediatric Heart Transplant Patients Using Machine Learning"
bibliography: refs.bib
csl: jacc.csl
output: 
  officedown::rdocx_document:
    reference_docx: style_manuscript_times_new_roman.docx
---

```{r setup, include=FALSE}

library(flextable)
library(glue)
library(table.glue)
library(stringr)
library(officer)
library(patchwork)
library(ggplot2)
library(magrittr)
library(tidyr)
library(dplyr, warn.conflicts = FALSE)
library(purrr)
library(Hmisc)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE
)

conflicted::conflict_prefer('summarize', 'dplyr')
conflicted::conflict_prefer('filter', 'dplyr')
source('../R/cmp_incidence.R')

knitr::opts_chunk$set(echo = FALSE)

drake::loadd()

```

# METHODS

## Modeling algorithms

We define the term 'modeling algorithm' as the collection of steps taken to develop a prediction model. We compared modeling algorithms based on the following characteristics: (1) number of predictors used in the model, (2) method to select the specified number of predictors, (3) method to develop a prediction model using the selected predictors. We considered seven options for the number of predictors, three options for variable selection, and four options for model development (Figure 1). All possible combinations of these options were assessed, leading to the evaluation of 84 unique modeling algorithms. 

__Methods to select predictors:__

We assess contribution importance, permutation importance, and stepwise importance. Contribution importance represents fractional contribution of each predictor to the model based on the total gain of this feature's splits [@friedman2001greedy]. Higher percentage means a more important predictive feature. Permutation importance measures the importance of each predictor by measuring how much the model's prediction error increases when the values of the predictor are randomly permuted [@breiman2001random]. Stepwise importance follows the traditional stepwise technique that includes or excludes variables one at a time until a stopping criteria is met [@zhang2016variable]. Though it is not a valid procedure for statistical inference, stepwise modeling algorithms can be useful for prediction [@freedman1992problem]. There is no numeric importance value for stepwise importance but a subset of the $k$ most important variables is easily obtained by using a pre-specified number of steps. 

<!-- In the current analysis, each variable importance technique leveraged a specific model: contribution importance was computed using a gradient boosted decision tree ensemble, permutation importance was computed using a random forest, and stepwise importance was computed using a Cox proportional hazards model. However, the modeling method used for variable selection was independent from the modeling method used for prediction. For example, a modeling algorithm may use contribution importance (i.e., gradient boosting) to select variables and then fit a Cox proportional hazards model using the selected variables to predict risk.  -->

__Methods to develop a risk prediction model:__

Proportional hazards regression is a semi-parametric modeling technique that estimates a baseline hazard function and assumes predictors have a multiplicative effect on the hazard for an event of interest. Gradient boosting (hereafter referred to as boosting) develops an ensemble of weak prediction models [@friedman2001greedy]. Each learner in the ensemble attempts to correct errors from the previous learners, and the ensemble's prediction is the aggregate of its individual learners' predictions. We developed boosting models using decision trees as learners [@breiman1984classification], a technique that has been recognized in numerous settings to be state of the art for statistical prediction [@chen2016xgboost]. Random forests are also ensembles of decision trees, but in contrast to boosting, the random forest comprises de-correlated decision trees that are grown using bootstrapped replicates of the original data [@breiman1996bagging; @breiman2001random]. We applied the standard random forest algorithm for risk prediction, which grows decision tree nodes by splitting the data based on an individual variable [@ishwaran2008random]. We also applied an extension of the random forest that splits data using linear combinations of variables [@jaeger2019oblique]. Random forests grown using linear combinations of input variables are referred to as 'oblique' random forests.  

<br>

## Internal validation

We applied Monte-Carlo cross-validation to identify the most effective modeling algorithm for developing a final prediction model and to estimate the performance of modeling algorithms when they are applied to new data [@xu2001monte]. Monte-Carlo cross-validation is an extension of split-sample testing, which splits available data into training and testing sets, then develops a model using the training set and validates the model in the testing set. Monte-Carlo cross-validation replicates this procedure, using a different split of the available data for each replicate, leading to a reduction in the variance of performance estimates for the modeling algorithms assessed [@steyerberg2001internal]. In the current analysis, we completed a total of 500 replications of split-sample testing. 

<br>

## Measures of model performance

Model performance was evaluated based on discrimination and calibration, as recommended by published guidelines [@steyerberg2010assessing; @steyerberg2014towards]. Discrimination and calibration were measured using a time-dependent concordance (C-) statistic and modified D'Agostino-Nam test, respectively [@blanche2013estimating; @demler2015tests]. Point estimates and 95% confidence intervals (CIs) for these metrics were computed empirically using the distribution of results from Monte-Carlo cross-validation. Point estimates were the median value of a performance metric, while the 2.5^th^ and 97.5^th^ percentiles defined lower and upper bounds for 95% CIs, respectively. We did not find previously published prediction models for graft loss among pediatric patients, so we did not analyze net reclassification improvement as it requires a baseline prediction model [@pencina2008evaluating]. 

<br>

## Missing data

Missing values were imputed after data were split into training and testing sets during each replicate of Monte-Carlo cross-validation. All information from the testing data was withheld during this process [@james2013introduction]. Specifically, missing values were imputed in both the training and testing data using the mean and mode of continuous and categorical variables, respectively, computed using the training data. While imputation to the mean is not appropriate for statistical inference, this technique has been shown to produce prediction models with Bayes consistency when missing values are non-informative [@josse2019consistency].

<br>

## Bayesian analysis of model performance

We applied Bayesian hierarchical models to draw inferences regarding the expected performance of modeling algorithms, accounting for correlated performance within each replicate of Monte-Carlo cross-validation [@benavoli2017time]. Specifically, we estimated the posterior probability that a given technique to fit a risk prediction model had superior discrimination or calibration compared to other techniques. All comparisons were made holding the number of predictor variables and the method to select predictor variables fixed; i.e., both models used the same predictor variables.

<br>

## Statistical analysis

We conducted the current analysis following previously published guidelines on multivariable prediction models for individual prognosis and diagnosis [@moons2015transparent]. The 1 year incidence of graft loss or morality was computed overall and by transplant year. Characteristics of patients were calculated as mean with standard deviation or percent in the overall population and stratified by transplant year. Using the results from internal validation, we visually assessed the discrimination and calibration of candidate modeling algorithms to develop a final prediction model. We used posterior predicted probability to perform inference on pairwise comparisons between candidate modeling algorithms. To make results relevant for clinical settings, we restricted these pairwise comparisons to models that used 20 predictor variables. Based on the results, we selected a final prediction modeling algorithm and applied it to all the available data. We tabulated a summary of the variables included into the final prediction model - including the numeric importance value and rank along with the count and percent of how many missing values were present. We used partial dependence to estimate multivariable adjusted predicted risk as a function of each variable included in the final prediction model, separately [@molnar2020interpretable].

<!-- We leveraged visual summaries of performance to select one modeling algorithm to develop a final prediction model using all available data. For this modeling algorithm, we tabulated discrimination and calibration from Monte-Carlo cross validation in the overall population and in subgroups defined by age, diagnosis group, and sex.  -->

```{r}
r_version <- glue::glue("{version$major}.{version$minor}")
```

<br>

Analyses were conducted using SAS version 9.4, R version `r r_version`, and a number of open-source R packages [@r_language; @table.glue; @drake; @tidymodels; @tidyverse]. All R code for the current analysis is publicly available at https://github.com/bcjaeger/graft-loss. Data for the current analysis are available by request from __FILL IN__.

<br>


# RESULTS

```{r, include=FALSE}


phts_outcome_truncated <- phts_all %>% 
  transmute(
    time_1yr = pmin(time, 1),
    status_1yr = if_else(time > 1, true = 0, false = status),
    txpl_year
  )

phts_nested_overall <- phts_outcome_truncated %>% 
  select(-txpl_year) %>% 
  nest(data = c(time_1yr, status_1yr)) %>% 
  mutate(group = 'overall', .before = data)

phts_nested_by_year <- phts_outcome_truncated %>% 
  nest(data = c(time_1yr, status_1yr)) %>% 
  transmute(group = as.character(txpl_year),
            data = data)

inline_incidence <- bind_rows(phts_nested_overall,
                              phts_nested_by_year) %>% 
  mutate(incidence = map(.x = data, 
                         .f = cmp_incidence,
                         time = 'time_1yr', 
                         status = 'status_1yr')) %>% 
  unnest_wider(col = incidence) %>% 
  mutate(
    x = table_glue("{incidence_est} ({incidence_lwr}, {incidence_upr})"),
    type = case_when(
      group == 'overall' ~ 'overall',
      incidence_est == min(incidence_est) ~ 'min',
      incidence_est == max(incidence_est) ~ 'max'
    )
  ) %>% 
  select(type, year = group, x) %>% 
  drop_na() %>% 
  as_inline(tbl_variables = c('type'),
            tbl_values = c('year', 'x'))

inline_auc <- fig_mc_cv_vals$AUC$data %>% 
  transmute(
    across(where(is.factor), as.character),
    across(where(is.character), str_replace_all, ' ', '_'),
    across(where(is.character), str_replace_all, '\n', '_'),
    across(where(is.character), tolower),
    n_predictors = paste("pred", n_predictors, sep = '_'),
    value = table_glue("{value_est} ({value_lwr}, {value_upr})")) %>% 
  as_inline(tbl_variables = c('model', 'ftr_selector', 'name', 'n_predictors'),
            tbl_values = 'value')

inline_cal <- fig_mc_cv_vals$GND.pvalue$data %>% 
  transmute(
    across(where(is.factor), as.character),
    across(where(is.character), str_replace_all, ' ', '_'),
    across(where(is.character), str_replace_all, '\n', '_'),
    across(where(is.character), tolower),
    n_predictors = paste("pred", n_predictors, sep = '_'),
    value = table_glue("{value_est} ({value_lwr}, {value_upr})")) %>% 
  as_inline(tbl_variables = c('model', 'ftr_selector', 'name', 'n_predictors'),
            tbl_values = 'value')


inline_winners <- 
  list(auc = fig_mc_cv_vals$GND.pvalue$data,
       cal = fig_mc_cv_vals$AUC$data) %>% 
  map(~ .x %>% 
        group_by(n_predictors, ftr_selector) %>% 
        arrange(desc(value_est)) %>% 
        slice(1) %>% 
        ungroup() %>% 
        count(model) %>% 
        mutate(model = tolower(str_replace_all(model, ' |\\n', '_')))) %>% 
  map(as_inline,
      tbl_variables = c("model"),
      tbl_values = 'n')


```

<br>

Overall, the 1-year incidence of graft loss or mortality after transplant was `r inline_incidence$overall$x`. The incidence was highest during `r inline_incidence$max$year` and lowest during `r inline_incidence$min$year`, with incidence rates (95% CI) of `r inline_incidence$max$x` and `r inline_incidence$min$x`, respectively. The overall sample was `r tbl_one$inline$sex$Overall$male` male, `r tbl_one$inline$race$Overall$white` white, and had mean (standard deviation) age of  `r tbl_one$inline$age_listing$Overall$level_missing` years at the time of transplant (__Table 1__). 

<br>

## Internal validation

Internally validated estimates of the C-statistic showed a progressive increase in model discrimination as more predictors were included, except when proportional hazards regression was applied to develop the risk prediction model (__Figure 2__). The oblique random survival forest obtained the highest C-statistic in `r inline_winners$auc$oblique_random_survival_forest` of 21 comparisons, and obtained the highest overall C-statistic when 15 or more predictor variables were selected, regardless of the variable selection method.  

<br>

With the exception of boosting, all modeling algorithms obtained adequate calibration (i.e., p-value for mis-calibration ≥ 0.05; __Figure 3__). The oblique random survival forest obtained the highest p-value for mis-calibration in `r inline_winners$cal$oblique_random_survival_forest` of 21 comparisons. When predictors were selected using permutation importance, the oblique random survival forest obtained the first and second highest median p-values for mis-calibration using 35 and 20 predictor variables, respectively.

__Bayesian analysis of model performance__: As discrimination and calibration were generally higher when permutation importance was applied, we focused our pairwise comparisons to models developed using 20 predictors selected by permutation importance (__Figure 4__). In this setting, oblique random survival forests obtained the highest C-statistic (`r fig_mc_cv_inf$inline$plot_center$auc$orsf`) and p-value for mis-calibration (`r fig_mc_cv_inf$inline$plot_center$gnd$orsf`). The posterior probability that oblique random survival forests obtained superior discrimination versus other modeling algorithms ranged from `r round(fig_mc_cv_inf$inline$plot_sides$auc$xgb$orsf$prob_gt_0_numeric,2)` (versus boosting) to `r round(fig_mc_cv_inf$inline$plot_sides$auc$cph$orsf$prob_gt_0_numeric,2)` (versus proportion hazards). The posterior probability that oblique random survival forests obtained superior calibration versus other modeling algorithms ranged from `r round(fig_mc_cv_inf$inline$plot_sides$gnd$orsf$rsf$prob_gt_0_numeric,2)` (versus standard random forests) to `r round(fig_mc_cv_inf$inline$plot_sides$gnd$orsf$xgb$prob_gt_0_numeric,2)` (versus boosting). 

__Selection of the final modeling algorithm__: To maintain clinical relevance, we limited the number of predictors in our final model to 20. Under this setting, using permutation importance to select predictors and oblique random survival forests to fit a risk prediction provided the best discrimination (C = `r bracket_drop(inline_auc$oblique_random_survival_forest$predictors_selected_by_permutation_importance$auc$pred_20)` and calibration (P-value for mis-calibration = `r bracket_drop(inline_cal$oblique_random_survival_forest$predictors_selected_by_permutation_importance$gnd.pvalue$pred_20)`). Therefore, we fit our final prediction model using the oblique random survival forest after selecting 20 predictors using permutation importance. 

<br>

## Model summary

The three variables with greatest permutation importance were cardiopulmonary bypass time, primary etiology (cardiomyopathy, congenital heart disease, or other), and ECMO at transplant (__Table 2__). Among the variables selected, the percentage of values missing ranged from 0 to `r tbl_variables$inline$chd_hlh$pct_miss`, and the overall percentage of values missing in the training data was `r table_value(mean(as.numeric(tbl_variables$table$body$dataset$pct_miss)))` 

```{r}

variable_importance <- final_features$importance %>% 
  separate(name, into = c('variable', 'category'), sep = '\\.\\.',
           remove = FALSE, fill = 'right')

inline_variables_final_model <- labels$variables %>% 
  right_join(variable_importance) %>% 
  mutate(label = str_remove(label, '^F\\dT |^F\\d ')) %>% 
  arrange(desc(value))

```


\newpage
<!-- TABLE 1: characteristics of participants for all variables -->
__Table 1:__ Characteristics of patients included in the current analysis.
`r fp_par(line_spacing = 1)`
`r tbl_one$table`
\newpage

<!---BLOCK_LANDSCAPE_START--->
<!-- TABLE 2: Final model; variable missingness and importance -->
__Table 2:__ Variable missingness and importance for predictors used in the final prediction model.
`r fp_par(line_spacing = 1)`
`r tbl_variables$table`
<!---BLOCK_LANDSCAPE_STOP--->


<!-- TABLE 3: Final model; variable dependence -->
__Table 3:__ Predicted risk for graft loss or mortality at 1 year following transplant according to the 10 most important variables selected for inclusion in the final prediction model.
`r fp_par(line_spacing = 1)`
`r tbl_partial_main$table`
\newpage


<!-- FIGURE 1: Diagram  -->
__Figure 1:__ Model development algorithm considered in the current analysis. 
```{r, out.width='100%'}
knitr::include_graphics('../fig/recipe.png')
```

<!-- FIGURE 2: C-stat  -->
\newpage
__Figure 2:__ Internally validated estimates of model discrimination as a function of the number of predictor variables included in the prediction model.
`r fp_par(line_spacing = 1)`

```{r, fig.width=6, fig.height=6.5, dpi=300}
fig_mc_cv_vals$AUC
```

Each point in the figure is the median value of model performance aggregated across 500 replicates of Monte-Carlo cross validation. 
`r fp_par(line_spacing = 1)`

Discrimination was assessed at 1 year after transplant using __FILL IN__.
`r fp_par(line_spacing = 1)`

<!-- FIGURE 3: Calibration  -->
\newpage
__Figure 3:__ Internally validated estimates of model calibration as a function of the number of predictor variables included in the prediction model.
`r fp_par(line_spacing = 1)`

```{r, fig.width=6, fig.height=6.5, dpi=300}
fig_mc_cv_vals$GND.pvalue
```

Model performance was defined using the p-value from a statistical test for miscalibration (REF).
`r fp_par(line_spacing = 1)`

Each point in the figure is the median value of model performance aggregated across 500 replicates of Monte-Carlo cross validation. 
`r fp_par(line_spacing = 1)`

Calibration was assessed at 1 year after transplant using __FILL IN__.
`r fp_par(line_spacing = 1)`

<!-- FIGURE 4: Inference  -->
\newpage
__Figure 4:__ Bayesian estimates of differences in model performance when 20 variables were selected using permutation importance.
`r fp_par(line_spacing = 1)`

```{r, fig.width=6, fig.height=6.5, dpi=300}
fig_mc_cv_inf$figure
```

<!---BLOCK_LANDSCAPE_START--->
__Figure 5:__ Incidence of graft loss or mortality, stratified by the four variables with greatest permuation importance in the final prediction model.
`r fp_par(line_spacing = 1)`

```{r, fig.width=9, fig.height=6, dpi=300}
fig_final_features
```
<!---BLOCK_LANDSCAPE_STOP--->

# SUPPLEMENT

\newpage

__Table S1__: Summary of all continuous candidate predictor variables.
`r fp_par(line_spacing = 1)`
`r tbl_predictor_smry$continuous`
\newpage

__Table S2__: Summary of all categorical candidate predictor variables.
`r fp_par(line_spacing = 1)`
`r tbl_predictor_smry$categorical`
\newpage

__Table S3__: Predicted risk for graft loss or mortality at 1 year following transplant according to variables with rank 11th or higher importance selected for inclusion in the final prediction model.
`r fp_par(line_spacing = 1)`
`r tbl_partial_supp$table`

\newpage
# REFERENCES
